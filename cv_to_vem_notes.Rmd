---
title: "CV를 넘어 VEM으로: 커널 밀도 추정의 원리와 완전 자동화"
author: "tae"
date: "`r format(Sys.Date())`"
output:
  html_document:
    toc: true
    toc_depth: 3
    number_sections: true
    df_print: paged
    code_folding: hide
    theme: united
fontsize: 11pt
geometry: margin=1in
always_allow_html: true
---

```{r setup, include=FALSE}
# R 청크의 전역 옵션을 설정합니다.
knitr::opts_chunk$set(
  echo = TRUE,      # 코드를 출력에 포함
  message = FALSE,  # 메시지 출력 안 함
  warning = FALSE,  # 경고 출력 안 함
  fig.align = "center", # 그림을 중앙 정렬
  fig.width = 8,
  fig.height = 5,
  dpi = 200,        # 그림 해상도
  dev = "png"       # 그래픽 장치
)
# 재현성을 위해 시드 설정
set.seed(123)
```

# 1. 서론: 왜 자동화된 밀도 추정이 중요한가?

데이터의 기저에 있는 확률 밀도 함수를 추정하는 것은 머신러닝의 핵심 과제입니다. 이는 **이상치 탐지**(분포에서 벗어난 데이터를 찾는 것), **생성 모델**(새로운 데이터를 만들어내는 것), **데이터 시각화** 등 다양한 분야에 활용됩니다. **커널 밀도 추정(Kernel Density Estimation, KDE)**, 특히 **커널 지수족(Kernel Exponential Family, KEF)** 모델은 복잡한 분포를 유연하게 모델링할 수 있는 강력한 도구입니다.

하지만 이 강력함에는 대가가 따릅니다. 바로 **하이퍼파라미터 튜닝**의 문제입니다.

* **σ (시그마):** 커널의 **대역폭(bandwidth)**으로, 모델이 얼마나 '부드러운' 분포를 학습할지 결정합니다. 너무 작으면 과적합되고, 너무 크면 과하게 단순화됩니다.
* **λ (람다):** **정규화(regularization)** 강도로, 모델의 복잡도를 제어하여 과적합을 방지합니다.

전통적으로 이러한 파라미터들은 **교차 검증(Cross-Validation, CV)**을 통해 탐색합니다. 하지만 이는 파라미터들의 모든 조합에 대해 모델을 수십, 수백 번씩 재학습해야 하는, 엄청난 계산 비용을 요구하는 고된 작업입니다. 😫

**이 강의 노트의 목표는** 이 고된 CV 과정을 **변분 기댓값-최대화(Variational EM, VEM)**라는 훨씬 더 지능적이고 효율적인 프레임워크로 완전히 대체하는 방법을 배우는 것입니다. VEM을 통해 **σ와 λ 같은 핵심 하이퍼파라미터들을 데이터로부터 자동으로 학습**시켜, 전체 튜닝 과정을 하나의 통합된 최적화 문제로 풀어내는 여정을 함께 떠나보겠습니다.

---

# 2. 이론적 배경: 스코어 매칭과 베이지안 프레임워크

### 2.1. 스코어 매칭: 다루기 힘든 적분 문제의 우회

KEF 모델의 로그 밀도는 \( \log p_\theta(x) = f_\theta(x) - A(\theta) \) 입니다. 여기서 \( f_\theta(x) = \mathbf{w}^\top \bm{\phi}_\sigma(x) \) 이고, \( A(\theta) \)는 로그 정규화 상수입니다. 이 \( A(\theta) = \log \int \exp(f_\theta(x)) dx \) 항은 전체 데이터 공간에 대한 적분을 포함하기 때문에 계산이 거의 불가능합니다.

**스코어 매칭(Score Matching)**은 이 문제를 우회하는 기법입니다. "밀도 함수 자체를 맞추기 어렵다면, 밀도 함수의 **기울기(gradient)**라도 맞추자!"는 아이디어입니다.  데이터 분포의 '스코어'(\( \nabla_x \log p(x) \))와 모델의 스코어(\( \nabla_x \log p_\theta(x) \)) 사이의 차이를 최소화하는 목적 함수를 사용하며, 놀랍게도 이 과정에서 다루기 힘든 \( A(\theta) \)가 사라집니다.

최종적으로 우리가 최소화할 스코어 매칭 목적 함수는 가중치 `w`에 대한 **이차식(Quadratic form)**이 됩니다.
$$
J(\mathbf{w};\sigma) = \frac12 \mathbf{w}^\top K_\sigma \mathbf{w} - \mathbf{b}_\sigma^\top \mathbf{w} + \text{상수}
$$
이 '이차식'이라는 특성이 VEM 알고리즘 적용의 열쇠가 됩니다.

### 2.2. 베이지안 모델링: 파라미터를 확률 변수로 보기

이제 관점을 전환하여, 이 문제를 베이지안 프레임워크에서 바라보겠습니다.

* **가능도 (Likelihood):** 스코어 매칭 목적 함수를 **음의 로그 가능도**로 간주합니다. 즉, \( J(\mathbf{w};\sigma) \)가 작을수록 데이터가 나타날 확률이 높다고 보는 것입니다.
    $$p(D \mid \mathbf{w}, \sigma, \beta) \propto \exp( -\beta J(\mathbf{w};\sigma) )$$
    - `β`는 모델의 예측(스코어)을 얼마나 신뢰할지를 나타내는 **정밀도(precision)** 파라미터입니다.

* **사전 분포 (Prior):** 모델이 너무 복잡해지는 것을 막기 위해 가중치 `w`에 대한 사전 믿음을 설정합니다. `w`의 값들이 0에 가까울 것이라는 가정을 하는 것은 전통적인 L2 정규화(Ridge)와 동일한 효과를 줍니다.
    $$p(\mathbf{w} \mid \tau) = \mathcal{N}(\mathbf{w} \mid \mathbf{0}, \tau^2 I)$$
    - `τ²`는 가중치가 0에서 얼마나 퍼져도 되는지를 나타내는 **분산**입니다. `1/τ²`가 바로 우리가 튜닝하던 정규화 강도 `λ`에 해당합니다. **`τ`가 작을수록 강한 정규화**를 의미합니다.

### 2.3. 증거 하한 (ELBO): 우리의 최종 목표 함수

우리가 진짜 최대화하고 싶은 것은 **모델 증거(Model Evidence)**, \( p(D | \sigma, \tau, \beta) \) 입니다. 이는 주어진 하이퍼파라미터 하에서 데이터가 나타날 총 확률로, 이 값이 높은 하이퍼파라미터가 가장 좋은 것입니다. 하지만 이 또한 직접 계산이 어렵습니다.

그래서 우리는 그 **하한(Lower Bound)**인 **증거 하한(ELBO)**을 대신 최대화합니다. ELBO를 최대화하면 실제 모델 증거도 함께 증가하는 효과가 있습니다. VEM은 바로 이 ELBO를 최대화하는 알고리즘입니다.

$$
\mathcal{L}(\mathbf{m}, S, \sigma, \beta, \tau) = \underbrace{\mathbb{E}_q[\log p(D \mid \mathbf{w}, \sigma, \beta)]}_{\text{데이터 적합도}} + \underbrace{\mathbb{E}_q[\log p(\mathbf{w} \mid \tau)] - \mathbb{E}_q[\log q(\mathbf{w})]}_{\text{모델 복잡도 제어 (정규화)}}
$$
여기서 \( q(\mathbf{w}) = \mathcal{N}(\mathbf{w} \mid \mathbf{m}, S) \)는 우리가 다루기 힘든 실제 사후 분포 \( p(\mathbf{w}|D) \)를 근사하기 위해 도입한 간단한 가우시안 분포입니다.

---

# 3. VEM 알고리즘: E-단계와 M-단계의 댄스 🕺💃

VEM은 ELBO를 최대화하기 위해 **E(Expectation) 단계**와 **M(Maximization) 단계**를 번갈아 수행합니다. 이는 마치 두 명의 댄서가 서로에게 맞춰가며 춤을 완성하는 것과 같습니다.

### 3.1. E-단계: 가중치(w)에 대한 믿음 업데이트

**"현재 하이퍼파라미터(σ, τ, β)가 주어졌을 때, 가중치 w는 어떤 분포를 따르는 것이 가장 합리적일까?"**

이 단계에서는 하이퍼파라미터를 상수로 고정하고, ELBO를 최대화하는 변분 파라미터 `m` (가중치의 평균)과 `S` (가중치의 공분산)를 찾습니다. 이는 근사 분포 `q(w)`가 실제 사후 분포에 가장 가까워지도록 업데이트하는 과정입니다.

* **S (공분산) 업데이트:**
    $$S \leftarrow (\beta K_\sigma + \tau^{-2} I)^{-1}$$
* **m (평균) 업데이트:**
    $$\mathbf{m} \leftarrow \beta S \mathbf{b}_\sigma$$

### 3.2. M-단계: 하이퍼파라미터 최적화

**"방금 계산한 가중치 분포 `q(w)`를 고려할 때, 이 데이터를 가장 잘 설명하는 하이퍼파라미터(σ, τ, β)는 무엇일까?"**

이 단계에서는 반대로 `m`과 `S`를 상수로 고정하고, ELBO를 최대화하는 하이퍼파라미터를 찾습니다.

* **τ (정규화) 업데이트:** `τ²`는 가중치의 평균 제곱 크기(`(tr(S) + ||m||²)/M`)로 업데이트됩니다. 즉, **데이터를 통해 모델에 필요한 최적의 정규화 강도를 매 단계마다 자동으로 결정**합니다.
    $$ \tau^2 \leftarrow \frac{\mathrm{tr}(S) + \|\mathbf{m}\|^2}{M} $$

* **β (정밀도) 업데이트:** `β`는 현재 모델의 예측 오차(스코어 매칭 손실)에 반비례하도록 업데이트됩니다. 모델이 데이터를 잘 설명하면 `β`가 커지고, 그렇지 않으면 작아집니다.
    $$ \beta \leftarrow \frac{2 \cdot \mathbf{b}_\sigma^\top \mathbf{m}}{\mathrm{tr}(K_\sigma S) + \mathbf{m}^\top K_\sigma \mathbf{m}} $$

* **σ (커널 대역폭) 업데이트:** `σ`는 닫힌 해가 없으므로 **경사 상승법(Gradient Ascent)**을 사용합니다. ELBO를 `σ`에 대해 미분하여, ELBO가 가장 가파르게 증가하는 방향으로 `σ`를 조금씩 업데이트합니다.
    $$ \sigma \leftarrow \sigma + \eta_\sigma \frac{\partial \mathcal{L}}{\partial \sigma} $$

이 두 단계를 반복하면, 파라미터들은 점차 안정된 값으로 수렴하고 ELBO는 꾸준히 증가하게 됩니다.

---

# 4. R 구현: VEM 알고리즘 실전 코딩

### 4.1. 데이터 생성

먼저, VEM 알고리즘을 테스트할 2차원 가우시안 혼합 데이터를 생성합니다.

```{r simulate-2d}
library(mvtnorm)
library(ggplot2)
library(gridExtra)

n  <- 200 # 계산 속도를 위해 샘플 수 조정
pi <- c(0.55, 0.45)
mu <- list(c(-1, -0.5), c(2, 1.2))
S1 <- matrix(c(0.6, 0.3, 0.3, 0.8), 2, 2)
S2 <- matrix(c(0.5, -0.2, -0.2, 0.5), 2, 2)
z  <- rbinom(n, size = 1, prob = pi[2]) + 1
X_data <- matrix(NA, n, 2)
X_data[z==1,] <- rmvnorm(sum(z==1), mean = mu[[1]], sigma = S1)
X_data[z==2,] <- rmvnorm(sum(z==2), mean = mu[[2]], sigma = S2)
dat <- data.frame(x1 = X_data[,1], x2 = X_data[,2])

# 초기 데이터 분포 확인
ggplot(dat, aes(x1, x2)) +
  geom_point(alpha = 0.8, size = 1.5, aes(color = as.factor(z))) +
  stat_density_2d(aes(fill = after_stat(level)), geom = "polygon", alpha = 0.2) +
  scale_fill_viridis_c() +
  scale_color_brewer(palette = "Set1") +
  coord_equal() +
  labs(title = "생성된 2D 데이터 및 실제 분포", x = "x1", y = "x2", color = "실제 클러스터") +
  theme_bw(base_size = 12) + theme(legend.position = "bottom")
```

### 4.2. 커널 및 미분 함수 정의

VEM 알고리즘의 각 단계에 필요한 가우시안 RBF 커널과 그 도함수들을 R 함수로 정의합니다.

```{r kernel-functions}
# 가우시안 RBF 커널 함수
rbf_kernel <- function(x, y, sigma) {
  exp(-sum((x - y)^2) / (2 * sigma^2))
}

# 커널의 x에 대한 그래디언트 (벡터 반환)
grad_x_rbf <- function(x, y, sigma) {
  -rbf_kernel(x, y, sigma) * (x - y) / sigma^2
}

# 커널의 x에 대한 라플라시안 (스칼라 반환)
lap_x_rbf <- function(x, y, sigma) {
  sq_dist <- sum((x - y)^2)
  d <- length(x)
  rbf_kernel(x, y, sigma) * (sq_dist / sigma^4 - d / sigma^2)
}

# --- M-단계의 sigma 업데이트를 위한 함수들 ---

# 커널의 sigma에 대한 그래디언트
grad_sigma_rbf <- function(x, y, sigma) {
  rbf_kernel(x, y, sigma) * sum((x-y)^2) / sigma^3
}

# 커널 그래디언트(grad_x_rbf)의 sigma에 대한 그래디언트
grad_sigma_grad_x_rbf <- function(x, y, sigma) {
  term1 <- grad_sigma_rbf(x, y, sigma) * (-(x-y)/sigma^2)
  term2 <- rbf_kernel(x, y, sigma) * (2*(x-y)/sigma^3)
  return(term1 + term2)
}

# 커널 라플라시안(lap_x_rbf)의 sigma에 대한 그래디언트
grad_sigma_lap_x_rbf <- function(x, y, sigma) {
  sq_dist <- sum((x - y)^2)
  d <- length(x)
  term1 <- grad_sigma_rbf(x,y,sigma) * (sq_dist / sigma^4 - d/sigma^2)
  term2 <- rbf_kernel(x,y,sigma) * (-4*sq_dist/sigma^5 + 2*d/sigma^3)
  return(term1 + term2)
}
```

### 4.3. VEM 알고리즘 구현: 핵심 루프

오류를 수정한 `compute_Kb` 함수와 전체 VEM 알고리즘 루프입니다. 각 단계에 상세한 주석을 추가하여 코드의 흐름을 쉽게 따라갈 수 있도록 했습니다.

```{r vem-implementation-fixed}
# K_sigma 와 b_sigma 및 그 도함수들을 계산하는 함수
compute_derivatives <- function(X, basis, sigma) {
  n <- nrow(X)
  M <- nrow(basis)
  D <- ncol(X)
  
  # 결과를 저장할 행렬들 초기화
  K_sigma <- matrix(0, M, M)
  b_sigma <- numeric(M)
  dK_dsigma <- matrix(0, M, M) # K_sigma의 sigma에 대한 도함수
  db_dsigma <- numeric(M)   # b_sigma의 sigma에 대한 도함수
  
  # 각 데이터 포인트에 대해 루프
  for (i in 1:n) {
    # 현 데이터 포인트 x_i에 대한 Jacobian 및 그 도함수 계산
    J_i <- matrix(0, D, M)         # J_i(j,k) = d(phi_k(x_i))/d(x_j)
    dJ_dsigma_i <- matrix(0, D, M) # d(J_i)/d(sigma)
    H_i_row <- numeric(M)          # 라플라시안 값들
    dH_dsigma_i_row <- numeric(M)  # 라플라시안의 sigma 도함수 값들
    
    for (j in 1:M) {
      J_i[, j] <- grad_x_rbf(X[i, ], basis[j, ], sigma)
      dJ_dsigma_i[, j] <- grad_sigma_grad_x_rbf(X[i, ], basis[j, ], sigma)
      H_i_row[j] <- lap_x_rbf(X[i, ], basis[j, ], sigma)
      dH_dsigma_i_row[j] <- grad_sigma_lap_x_rbf(X[i, ], basis[j, ], sigma)
    }
    
    # K_sigma 와 b_sigma 업데이트 (오류 수정된 부분)
    # J_i는 D x M 행렬, t(J_i)는 M x D 행렬. t(J_i) %*% J_i 는 M x M 행렬이 됨.
    K_sigma <- K_sigma + t(J_i) %*% J_i
    b_sigma <- b_sigma - H_i_row
    
    # 도함수 행렬 업데이트 (Product Rule 적용)
    dK_dsigma <- dK_dsigma + (t(dJ_dsigma_i) %*% J_i + t(J_i) %*% dJ_dsigma_i)
    db_dsigma <- db_dsigma - dH_dsigma_i_row
  }
  
  # 샘플 수 n으로 나누어 평균 계산
  return(list(
    K = K_sigma / n,
    b = b_sigma / n,
    dK_dsigma = dK_dsigma / n,
    db_dsigma = db_dsigma / n
  ))
}


# VEM 알고리즘 주 함수
run_vem <- function(X_data, basis_pts, n_iters = 50, lr_sigma = 0.01) {
  
  M <- nrow(basis_pts)
  
  # 1. 하이퍼파라미터 초기화
  sigma <- 1.0
  tau2 <- 1.0 # tau의 제곱으로 관리
  beta <- 1.0
  
  history <- data.frame(iter = 1:n_iters, elbo = NA, sigma = NA, tau = NA, beta = NA)
  
  cat("VEM 알고리즘 시작...\n")
  for (iter in 1:n_iters) {
    # --- M-단계 준비: 필요한 모든 도함수 계산 ---
    derivs <- compute_derivatives(X_data, basis_pts, sigma)
    K_sigma <- derivs$K
    b_sigma <- derivs$b
    
    # --- E-단계: m, S 업데이트 ---
    S <- solve(beta * K_sigma + (1/tau2) * diag(M))
    m <- beta * S %*% b_sigma
    
    # --- M-단계: 하이퍼파라미터 업데이트 ---
    
    # 1. tau 업데이트 (정규화 강도 자동 조절)
    tau2 <- (sum(diag(S)) + sum(m^2)) / M
    
    # 2. beta 업데이트 (모델 신뢰도 자동 조절)
    numerator <- 2 * t(b_sigma) %*% m
    denominator <- sum(diag(K_sigma %*% S)) + t(m) %*% K_sigma %*% m
    beta <- as.numeric(numerator / denominator)
    
    # 3. sigma 업데이트 (경사 상승법)
    # ELBO의 sigma에 대한 그래디언트 계산
    grad_L_sigma <- -0.5 * beta * (sum(diag(derivs$dK_dsigma %*% S)) + t(m) %*% derivs$dK_dsigma %*% m) +
                    beta * t(derivs$db_dsigma) %*% m
    
    sigma <- sigma + lr_sigma * grad_L_sigma
    sigma <- max(sigma, 0.01) # sigma가 음수가 되지 않도록 방지
    
    # --- 모니터링: ELBO 계산 ---
    log_det_S <- determinant(S, logarithm = TRUE)$modulus[1]
    elbo <- -0.5*beta*(sum(diag(K_sigma %*% S)) + t(m)%*%K_sigma%*%m) + 
            beta * t(b_sigma)%*%m - 
            0.5/tau2 * (sum(diag(S)) + sum(m^2)) + 0.5*log_det_S
    
    history[iter, ] <- c(iter, elbo, sigma, sqrt(tau2), beta)
    if (iter %% 10 == 0) cat(sprintf("  Iter %d: ELBO=%.2f, sigma=%.3f, tau=%.3f\n", iter, elbo, sigma, sqrt(tau2)))
  }
  cat("VEM 알고리즘 종료.\n")
  
  return(list(m = m, S = S, final_params = c(sigma, sqrt(tau2), beta), history = history))
}

# 기저점(basis points)으로 데이터 포인트를 그대로 사용
basis_pts <- X_data
# VEM 실행
vem_results <- run_vem(X_data, basis_pts, n_iters = 40, lr_sigma = 0.001)

# 최종 학습된 하이퍼파라미터 출력
print("최종 하이퍼파라미터 (sigma, tau, beta):")
print(vem_results$final_params)
```

### 4.4. 결과 시각화 및 심층 분석

VEM 알고리즘이 하이퍼파라미터들을 어떻게 학습했고, 그 결과로 얻어진 밀도 추정이 어떤 모습인지 시각적으로 분석해 보겠습니다.

```{r visualize-results}
# 1. 하이퍼파라미터 및 ELBO 수렴 과정 시각화
history <- vem_results$history
p1 <- ggplot(history, aes(x = iter, y = sigma)) + geom_line(color="blue") + geom_point(color="blue") + labs(title = "Sigma (커널 대역폭) 수렴 과정", y = "sigma") + theme_bw()
p2 <- ggplot(history, aes(x = iter, y = tau)) + geom_line(color="red") + geom_point(color="red") + labs(title = "Tau (정규화) 수렴 과정", y = "tau") + theme_bw()
p3 <- ggplot(history, aes(x = iter, y = elbo)) + geom_line(color="darkgreen") + geom_point(color="darkgreen") + labs(title = "ELBO (목표 함수) 수렴 과정", y = "ELBO") + theme_bw()

grid.arrange(p1, p2, p3, ncol = 3)

# 2. 최종 추정된 밀도 플롯
# 평가용 그리드 생성
grid_range <- apply(X_data, 2, function(x) range(x) + c(-1, 1))
x1_grid <- seq(grid_range[1,1], grid_range[2,1], length.out = 80)
x2_grid <- seq(grid_range[1,2], grid_range[2,2], length.out = 80)
eval_grid <- expand.grid(x1 = x1_grid, x2 = x2_grid)

# 그리드 각 점에서 f(x) 값 계산 (로그 밀도에 비례)
final_m <- vem_results$m
final_sigma <- vem_results$final_params[1]
log_density_vals <- apply(eval_grid, 1, function(x_eval) {
  phi_x <- sapply(1:nrow(basis_pts), function(j) rbf_kernel(x_eval, basis_pts[j,], final_sigma))
  sum(final_m * phi_x)
})

eval_grid$log_density <- log_density_vals

# 최종 밀도 시각화
ggplot() +
  stat_contour(data = eval_grid, aes(x = x1, y = x2, z = log_density, fill = ..level..), geom = "polygon", alpha=0.8) +
  geom_point(data = dat, aes(x = x1, y = x2), alpha = 0.7, size = 1.5, shape=3, color="white") +
  scale_fill_viridis_c(option = "plasma") +
  coord_equal(xlim = grid_range[,1], ylim = grid_range[,2]) +
  labs(title = "VEM을 통해 최종적으로 학습된 밀도",
       subtitle = paste("최종 sigma =", round(final_sigma, 3), " / 최종 tau =", round(vem_results$final_params[2], 3)),
       x = "x1", y = "x2", fill = "로그 밀도") +
  theme_minimal(base_size = 14)

```

---

# 5. 결론: 똑똑한 자동화의 힘 🧠✨

이 강의 노트를 통해 우리는 VEM 알고리즘의 원리를 깊이 이해하고, 실제 코드로 구현하여 그 강력함을 확인했습니다.

1.  **완전한 자동화의 실현:** 우리는 `sigma`(커널 대역폭), `tau`(정규화 강도)와 같은 핵심 하이퍼파라미터들을 **교차 검증 없이**, 단일 최적화 루프 내에서 데이터로부터 직접 학습했습니다. 수렴 과정 그래프는 각 파라미터가 ELBO를 최대화하는 방향으로 안정적인 값에 도달했음을 명확히 보여줍니다.

2.  **이론과 실제의 완벽한 연결:** M-단계에서 유도된 분석적인 업데이트 규칙과 경사 상승법이 실제 코드에 어떻게 적용되는지, 특히 오류를 수정하고 도함수를 정확히 계산하는 과정을 통해 명확히 이해할 수 있었습니다.

3.  **결과의 타당성 및 해석:** 최종적으로 추정된 밀도 플롯은 초기 데이터의 이중 봉우리(bimodal) 분포를 성공적으로 포착했습니다. 이는 VEM이 복잡한 하이퍼파라미터 공간을 효과적으로 탐색하여 타당한 모델을 찾아냈음을 의미합니다. 최종적으로 학습된 `sigma`와 `tau` 값은 이 데이터셋에 적합한 '부드러움'과 '복잡도'를 VEM 스스로 찾아냈음을 보여줍니다.

결론적으로, VEM은 교차 검증의 계산적 비효율성과 번거로움을 극복하고, **더 빠르고, 더 원칙에 입각한 방식**으로 비모수적 밀도 추정 모델을 구축할 수 있는 강력한 대안입니다. 이 프레임워크는 베이지안 추론의 장점인 불확실성 정량화와 자동화된 파라미터 튜닝을 동시에 제공하여, 데이터 과학자에게 더 강력한 무기를 쥐여줍니다.